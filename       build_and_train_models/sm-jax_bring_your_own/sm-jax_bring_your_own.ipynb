{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Deploying ML Models using JAX on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ff9900; border-radius: 8px; padding: 15px; background-color: #fff3e0; margin-bottom: 10px;\">\n",
    "<strong>⚠️ Compatibility Notice:</strong> This notebook has been tested using <strong>SageMaker Distribution Image 3.7.0</strong> and the <strong>SageMaker Python SDK version 3.4.0</strong>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker provides you the flexibility to train models using our pre-built machine learning containers or your own bespoke container. We'll refer to these strategies as Bring-Your-Own-Script **(BYOS)** and Bring-Your-Own-Container **(BYOC)** in this tutorial. \n",
    "\n",
    "### Bring Your Own JAX Script\n",
    "\n",
    "In this notebook, we'll show how to extend our optimized TensorFlow containers to train machine learning models using the increasingly popular [JAX library](https://github.com/google/jax). We'll train a fashion MNIST classification model using vanilla JAX, another using `jax.experimental.stax`, and a final model using the [higher level Trax library](https://github.com/google/trax).\n",
    "\n",
    "For all three patterns, we'll show how the JAX models can be serialized as standard TensorFlow [SavedModel format](https://www.tensorflow.org/guide/saved_model). This enables us to seamlessly deploy the models using the managed and optimized SageMaker TensorFlow inference containers.\n",
    "\n",
    "\n",
    "### Bring Your Own JAX Container\n",
    "\n",
    "We've included a dockerfile in this repo directory to show how you can build your own bespoke JAX container with support for GPUs on SageMaker. Unfortunately, the NVIDIA/CUDA Dockerhub containers have a [deletion policy](https://gitlab.com/nvidia/container-images/cuda/blob/master/doc/support-policy.md), so we're unable to assert that the container can be built through time. Nonetheless, you can trivially adapt a newer version of the container if your workload requires a custom container. For more information on running BYOC on SageMaker see the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-training-container.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U \"sagemaker==3.4.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from importlib.metadata import version as pkg_version\n",
    "from sagemaker.core.helper.session_helper import get_execution_role\n",
    "from sagemaker.core import image_uris\n",
    "from sagemaker.train import ModelTrainer\n",
    "from sagemaker.train.configs import SourceCode, Compute\n",
    "\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "sm_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "print(f\"SageMaker SDK version: {pkg_version('sagemaker')}\")\n",
    "print(f\"Role: {role}\")\n",
    "print(f\"Region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve TF 2.10 training image (GPU) and inference image (CPU)\n",
    "tf_training_image = image_uris.retrieve(\n",
    "    framework=\"tensorflow\", region=region, version=\"2.10\",\n",
    "    instance_type=\"ml.g5.xlarge\", image_scope=\"training\",\n",
    ")\n",
    "tf_inference_image = image_uris.retrieve(\n",
    "    framework=\"tensorflow\", region=region, version=\"2.10\",\n",
    "    instance_type=\"ml.m5.xlarge\", image_scope=\"inference\",\n",
    ")\n",
    "print(f\"Training image: {tf_training_image}\")\n",
    "print(f\"Inference image: {tf_inference_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing JAX in SageMaker TensorFlow Containers\n",
    "\n",
    "When using BYOS with managed SageMaker containers, you can trivially install extra dependencies by providing a `requirements.txt` within the `source_dir` that contains your training scripts. With `ModelTrainer`, specify the `requirements` parameter in `SourceCode` to ensure dependencies are installed prior to executing the training script.\n",
    "\n",
    "To be specific, any container that has the [sagemaker-training-toolkit](https://github.com/aws/sagemaker-training-toolkit) supports installing additional dependencies from `requirements.txt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serializing models as SavedModel format\n",
    "In the upcoming training jobs we'll be training a vanilla JAX model, a Stax model, and a Trax model on the [fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist).\n",
    "The full details of the model can be seen in the `training_scripts/` directory, but it is worth calling out the methods for serialization.\n",
    "\n",
    "The JAX/Stax models utilize the new jax2tf converter: https://github.com/google/jax/tree/master/jax/experimental/jax2tf\n",
    "\n",
    "```python\n",
    "def save_model_tf(prediction_function, params_to_save):\n",
    "    tf_fun = jax2tf.convert(prediction_function, enable_xla=False)\n",
    "    param_vars = tf.nest.map_structure(lambda param: tf.Variable(param), params_to_save)\n",
    "\n",
    "    tf_graph = tf.function(\n",
    "        lambda inputs: tf_fun(param_vars, inputs),\n",
    "        autograph=False,\n",
    "        jit_compile=False,\n",
    "    )\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "The Trax model utilizes the new trax2keras functionality: https://github.com/google/trax/blob/master/trax/trax2keras.py\n",
    "\n",
    "```python\n",
    "def save_model_tf(model_to_save):\n",
    "    \"\"\"\n",
    "    Serialize a TensorFlow graph from trained Trax Model\n",
    "    :param model_to_save: Trax Model\n",
    "    \"\"\"\n",
    "    keras_layer = trax.AsKeras(model_to_save, batch_size=1)\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "    hidden = keras_layer(inputs)\n",
    "\n",
    "    keras_model = tf.keras.Model(inputs=inputs, outputs=hidden)\n",
    "    keras_model.save(\"/opt/ml/model/1\", save_format=\"tf\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using Vanilla JAX\n",
    "\n",
    "Note: Our `source_dir` directory contains a `requirements.txt` that will install JAX with CUDA support. We use thh `ml.g5.xlarge` (A10G GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_jax_trainer = ModelTrainer(\n",
    "    training_image=tf_training_image,\n",
    "    source_code=SourceCode(\n",
    "        source_dir=\"training_scripts\",\n",
    "        entry_script=\"train_jax.py\",\n",
    "        requirements=\"requirements.txt\",\n",
    "    ),\n",
    "    compute=Compute(instance_type=\"ml.g5.xlarge\", instance_count=1),\n",
    "    role=role,\n",
    "    base_job_name=\"jax\",\n",
    "    hyperparameters={\"num_epochs\": \"3\"},\n",
    ")\n",
    "vanilla_jax_trainer.train(wait=True, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Using JAX Medium-level API Stax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stax_trainer = ModelTrainer(\n",
    "    training_image=tf_training_image,\n",
    "    source_code=SourceCode(\n",
    "        source_dir=\"training_scripts\",\n",
    "        entry_script=\"train_stax.py\",\n",
    "        requirements=\"requirements.txt\",\n",
    "    ),\n",
    "    compute=Compute(instance_type=\"ml.g5.xlarge\", instance_count=1),\n",
    "    role=role,\n",
    "    base_job_name=\"stax\",\n",
    "    hyperparameters={\"num_epochs\": \"3\"},\n",
    ")\n",
    "stax_trainer.train(wait=True, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Using JAX High-level API Trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trax_trainer = ModelTrainer(\n",
    "    training_image=tf_training_image,\n",
    "    source_code=SourceCode(\n",
    "        source_dir=\"training_scripts\",\n",
    "        entry_script=\"train_trax.py\",\n",
    "        requirements=\"requirements.txt\",\n",
    "    ),\n",
    "    compute=Compute(instance_type=\"ml.g5.xlarge\", instance_count=1),\n",
    "    role=role,\n",
    "    base_job_name=\"trax\",\n",
    "    hyperparameters={\"train_steps\": \"1000\"},\n",
    ")\n",
    "trax_trainer.train(wait=True, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Models to managed TF Containers\n",
    "Since we've serialized the models as TensorFlow SavedModel format, deploying these models as endpoints uses `ModelBuilder` with the TF inference image and the S3 model artifacts from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serve import ModelBuilder\n",
    "from sagemaker.serve.mode.function_pointers import Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_jax_mb = ModelBuilder(\n",
    "    s3_model_data_url=vanilla_jax_trainer._latest_training_job.model_artifacts.s3_model_artifacts,\n",
    "    image_uri=tf_inference_image,\n",
    "    role_arn=role,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    mode=Mode.SAGEMAKER_ENDPOINT,\n",
    ")\n",
    "vanilla_jax_model = vanilla_jax_mb.build()\n",
    "vanilla_jax_endpoint = vanilla_jax_mb.deploy(\n",
    "    endpoint_name=\"jax-vanilla-v3\",\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stax_mb = ModelBuilder(\n",
    "    s3_model_data_url=stax_trainer._latest_training_job.model_artifacts.s3_model_artifacts,\n",
    "    image_uri=tf_inference_image,\n",
    "    role_arn=role,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    mode=Mode.SAGEMAKER_ENDPOINT,\n",
    ")\n",
    "stax_model = stax_mb.build()\n",
    "stax_endpoint = stax_mb.deploy(\n",
    "    endpoint_name=\"jax-stax-v3\",\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trax_mb = ModelBuilder(\n",
    "    s3_model_data_url=trax_trainer._latest_training_job.model_artifacts.s3_model_artifacts,\n",
    "    image_uri=tf_inference_image,\n",
    "    role_arn=role,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    mode=Mode.SAGEMAKER_ENDPOINT,\n",
    ")\n",
    "trax_model = trax_mb.build()\n",
    "trax_endpoint = trax_mb.deploy(\n",
    "    endpoint_name=\"jax-trax-v3\",\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Inference Endpoints\n",
    "This requires TF to be installed on your notebook's kernel as it is used to load testing data. Inference uses `boto3 invoke_endpoint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_image(endpoint_name, test_images, test_labels, image_number):\n",
    "    np_img = np.expand_dims(np.expand_dims(test_images[image_number], axis=-1), axis=0)\n",
    "    payload = json.dumps({\"instances\": np_img.tolist()})\n",
    "\n",
    "    response = sm_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/json\",\n",
    "        Accept=\"application/json\",\n",
    "        Body=payload,\n",
    "    )\n",
    "    result = json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n",
    "    pred_y = np.argmax(result[\"predictions\"])\n",
    "\n",
    "    print(\"True Label:\", test_labels[image_number])\n",
    "    print(\"Predicted Label:\", pred_y)\n",
    "    plt.imshow(test_images[image_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image(\"jax-vanilla-v3\", x_test, y_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image(\"jax-stax-v3\", x_test, y_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image(\"jax-trax-v3\", x_test, y_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Delete the running endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean-Up\n",
    "for ep_name in [\"jax-vanilla-v3\", \"jax-stax-v3\", \"jax-trax-v3\"]:\n",
    "    try:\n",
    "        sm_client.delete_endpoint(EndpointName=ep_name)\n",
    "        print(f\"Deleted endpoint: {ep_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete endpoint {ep_name}: {e}\")\n",
    "    try:\n",
    "        sm_client.delete_endpoint_config(EndpointConfigName=ep_name)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "\n",
    "1. Create a JAX training script\n",
    "2. Train models using `ModelTrainer` with the SageMaker TensorFlow training container\n",
    "3. Deploy models using `ModelBuilder` with the SageMaker TensorFlow inference container\n",
    "4. Test the deployed models via `boto3 invoke_endpoint`\n",
    "5. Clean up resources\n",
    "\n",
    "The JAX framework provides excellent performance for machine learning workloads, and SageMaker makes it easy to scale your JAX models in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Experiment with different JAX model architectures\n",
    "- Use real datasets instead of synthetic data\n",
    "- Implement more sophisticated preprocessing\n",
    "- Add model monitoring and logging\n",
    "- Explore distributed training with multiple instances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
