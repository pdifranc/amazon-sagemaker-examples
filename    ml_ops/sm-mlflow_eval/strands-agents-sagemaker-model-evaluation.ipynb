{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Strands Agents with SageMaker AI Models and MLflow\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook accompanies the blog post **\"Build Strands agents with SageMaker AI models and MLflow\"** and provides a hands-on guide to building AI agents using the Strands Agents SDK with models deployed on Amazon SageMaker AI endpoints, while leveraging SageMaker Managed MLflow for comprehensive agent observability.\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "1. **Understand Strands Agents SDK** - Build AI agents with just a few lines of code\n",
    "2. **Deploy Models on SageMaker AI** - Deploy foundation models from SageMaker JumpStart\n",
    "3. **Integrate Strands with SageMaker** - Use SageMaker-deployed models with Strands agents\n",
    "4. **Set Up Agent Observability** - Configure SageMaker Managed MLflow for agent tracing\n",
    "5. **Implement A/B Testing with Evaluation** - Deploy multiple model variants and evaluate with MLflow metrics\n",
    "6. **Evaluate Agents with Strands Framework** - Use Strands built-in evaluation for comprehensive testing\n",
    "\n",
    "### Why Use SageMaker AI Instead of Bedrock?\n",
    "\n",
    "- **Infrastructure Control**: Full control over compute instances, networking, and scaling\n",
    "- **Model Flexibility**: Deploy custom models, fine-tuned variants, or open-source alternatives\n",
    "- **Cost Predictability**: Precise cost forecasting through reserved instances\n",
    "- **Advanced MLOps**: Enterprise-grade model governance with MLflow integration\n",
    "\n",
    "**Estimated time: ~75 minutes** | ‚ö†Ô∏è Run cells sequentially from top to bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Prerequisites and Setup\n",
    "\n",
    "Before building our Strands agent, we need to set up our environment with the required packages.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- AWS Account with access to Amazon Bedrock and Amazon SageMaker AI\n",
    "- IAM role with access to SageMaker AI, Bedrock, MLflow, S3, and JumpStart\n",
    "- Jupyter notebook running locally or on SageMaker AI Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "strands-agents>=1.9.1\n",
    "strands-agents-tools>=0.2.8\n",
    "mlflow>=3.4.0\n",
    "strands-agents[sagemaker]\n",
    "pandas>=2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Building Your First Strands Agent\n",
    "\n",
    "Strands Agents SDK is an open source SDK that takes a model-driven approach to building and running AI agents. It combines:\n",
    "\n",
    "- **A Model**: The foundation model that powers the agent's reasoning\n",
    "- **A System Prompt**: Instructions that guide the agent's behavior\n",
    "- **Tools**: Capabilities the agent can use to interact with external systems\n",
    "\n",
    "Let's start with a simple agent using Amazon Bedrock to understand the basics before moving to SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from strands.models.bedrock import BedrockModel\n",
    "from strands import Agent\n",
    "from strands_tools import http_request\n",
    "\n",
    "# Create a model using Amazon Bedrock\n",
    "model = BedrockModel(\n",
    "    model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    ")\n",
    "\n",
    "# Create an agent with the model and tools\n",
    "agent = Agent(model=model, tools=[http_request])\n",
    "\n",
    "# Test the agent\n",
    "agent(\"Where is the international space station now?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Why Use Models Deployed on SageMaker AI?\n",
    "\n",
    "Organizations may choose to deploy foundation models on SageMaker AI for several reasons:\n",
    "\n",
    "| Benefit | Description |\n",
    "|---------|-------------|\n",
    "| **Infrastructure Control** | Full control over compute instances, networking configurations, and scaling policies. Crucial for organizations with strict latency SLAs or specific hardware requirements. |\n",
    "| **Model Flexibility** | Deploy any model - custom architectures, fine-tuned variants, or open-source alternatives like Llama or Mistral. |\n",
    "| **Cost Predictability** | Precise cost forecasting and optimization through reserved instances, spot pricing, and right-sized compute resources. |\n",
    "| **Advanced MLOps** | Integration with MLflow, model registry, and A/B testing capabilities for enterprise-grade model governance. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Deploy Model as SageMaker AI Endpoint\n",
    "\n",
    "Now we'll deploy a Qwen3-4B model from SageMaker JumpStart as an inference endpoint. SageMaker JumpStart provides pre-trained models that can be deployed with just a few lines of code.\n",
    "\n",
    "**Note**: Any model you use with Strands Agents SDK should support OpenAI compatible chat completions APIs.\n",
    "\n",
    "‚è±Ô∏è **This step takes approximately 5-10 minutes** to complete as the endpoint is being provisioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, botocore\n",
    "from boto3.session import Session\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "import sagemaker\n",
    "\n",
    "boto_session = Session()\n",
    "sts = boto3.client('sts')\n",
    "account_id = sts.get_caller_identity().get(\"Account\")\n",
    "region = boto_session.region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "ENDPOINT_NAME = INITIAL_CONFIG_NAME = \"llm-qwen-endpoint-sagemaker\"\n",
    "\n",
    "# Check if endpoint already exists\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "try:\n",
    "    endpoint_info = sagemaker_client.describe_endpoint(EndpointName=ENDPOINT_NAME)\n",
    "    endpoint_status = endpoint_info['EndpointStatus']\n",
    "    \n",
    "    if endpoint_status == 'InService':\n",
    "        print(f\"‚úÖ Endpoint '{ENDPOINT_NAME}' already exists and is InService. Reusing existing endpoint.\")\n",
    "    else:\n",
    "        print(f\"‚è≥ Endpoint '{ENDPOINT_NAME}' exists with status: {endpoint_status}. Waiting...\")\n",
    "        waiter = sagemaker_client.get_waiter('endpoint_in_service')\n",
    "        waiter.wait(EndpointName=ENDPOINT_NAME)\n",
    "        print(f\"‚úÖ Endpoint '{ENDPOINT_NAME}' is now InService.\")\n",
    "\n",
    "except (sagemaker_client.exceptions.ResourceNotFound, botocore.exceptions.ClientError):\n",
    "    # Endpoint doesn't exist, create it\n",
    "    print(f\"Endpoint '{ENDPOINT_NAME}' not found. Creating new endpoint...\")\n",
    "    \n",
    "    model_a = JumpStartModel(\n",
    "        model_id=\"huggingface-reasoning-qwen3-4b\", \n",
    "        model_version=\"1.0.0\",\n",
    "        name=\"qwen3-4b-model\"\n",
    "    )\n",
    "    \n",
    "    predictor_a = model_a.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=\"ml.g5.2xlarge\",\n",
    "        endpoint_name=ENDPOINT_NAME\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Endpoint '{ENDPOINT_NAME}' deployed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Building a Strands Agent with SageMaker AI Models\n",
    "\n",
    "With the model deployed, we can now create a `SageMakerAIModel` and use it with Strands Agents. The Strands Agents SDK implements a SageMaker provider that allows you to run agents against models deployed on SageMaker inference endpoints.\n",
    "\n",
    "Key configuration options:\n",
    "- **endpoint_name**: The name of your SageMaker endpoint\n",
    "- **region_name**: AWS region where the endpoint is deployed\n",
    "- **max_tokens**: Maximum tokens in the response\n",
    "- **temperature**: Controls randomness (lower = more deterministic)\n",
    "- **stream**: Enable streaming responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from strands.models.sagemaker import SageMakerAIModel\n",
    "from strands import Agent, tool\n",
    "from strands_tools import http_request, calculator\n",
    "\n",
    "model_sagemaker = SageMakerAIModel(\n",
    "    endpoint_config={\n",
    "        \"endpoint_name\": ENDPOINT_NAME,\n",
    "        \"region_name\": region\n",
    "    },\n",
    "    payload_config={\n",
    "        \"max_tokens\": 2048,\n",
    "        \"temperature\": 0.2,\n",
    "        \"stream\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Test the agent with SageMaker model\n",
    "agent = Agent(model=model_sagemaker, tools=[http_request], callback_handler=None)\n",
    "agent(\"Where is the international space station now? (Use: http://api.open-notify.org/iss-now.json)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Using SageMaker AI Serverless MLflow App for Agent Observability\n",
    "\n",
    "SageMaker AI Serverless MLflow provides comprehensive observability for AI agents by:\n",
    "\n",
    "- **Automatic Trace Capture**: Captures execution traces, tool usage patterns, and decision-making workflows\n",
    "- **No Custom Instrumentation**: Works out of the box with Strands Agents SDK\n",
    "- **Centralized Monitoring**: Monitor agent behavior across multiple deployments\n",
    "- **Audit Trails**: Maintain compliance requirements with detailed execution logs\n",
    "\n",
    "### Step 1: Create MLflow Tracking Server\n",
    "\n",
    "First, we'll create a S3 bucket to hold MLflow artifacts and then SageMaker Managed MLflow App using the SageMaker SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 bucket for MLflow artifacts\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "bucket_name = f'{account_id}-mlflow-bucket'\n",
    "\n",
    "try:\n",
    "    # Check if bucket exists\n",
    "    s3_client.head_bucket(Bucket=bucket_name)\n",
    "    print(f\"‚úÖ S3 bucket '{bucket_name}' already exists.\")\n",
    "except s3_client.exceptions.ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    if error_code == '404':\n",
    "        # Bucket doesn't exist, create it\n",
    "        print(f\"Creating S3 bucket '{bucket_name}'...\")\n",
    "        if region == 'us-east-1':\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "        print(f\"‚úÖ S3 bucket '{bucket_name}' created successfully!\")\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_APP_NAME = 'strands-mlflow-app'\n",
    "\n",
    "# Check if MLflow app already exists by listing apps\n",
    "existing_app = None\n",
    "try:\n",
    "    apps_response = sagemaker_client.list_mlflow_apps()\n",
    "    for app in apps_response.get('Summaries', []):\n",
    "        if app['Name'] == MLFLOW_APP_NAME:\n",
    "            existing_app = app\n",
    "            break\n",
    "except Exception as e:\n",
    "    print(f\"Error listing apps: {e}\")\n",
    "\n",
    "if existing_app:\n",
    "    # App exists, get details\n",
    "    mlflow_app_details = sagemaker_client.describe_mlflow_app(Arn=existing_app['Arn'])\n",
    "    status = mlflow_app_details['Status']\n",
    "    \n",
    "    if status == 'RUNNING':\n",
    "        print(f\"‚úÖ MLflow app '{MLFLOW_APP_NAME}' already exists and is running.\")\n",
    "    else:\n",
    "        print(f\"‚è≥ MLflow app '{MLFLOW_APP_NAME}' exists with status: {status}\")\n",
    "    \n",
    "    print(f\"   ARN: {existing_app['Arn']}\")\n",
    "    mlflow_app_details = {'Arn': existing_app['Arn']}\n",
    "else:\n",
    "    # MLflow app doesn't exist, create it\n",
    "    print(f\"MLflow app '{MLFLOW_APP_NAME}' not found. Creating new app...\")\n",
    "    \n",
    "    mlflow_app_details = sagemaker_client.create_mlflow_app(\n",
    "        Name=MLFLOW_APP_NAME,\n",
    "        ArtifactStoreUri=f's3://{account_id}-mlflow-bucket/artifacts',\n",
    "        RoleArn=role,\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ MLflow app creation initiated: {mlflow_app_details['Arn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow app takes few minutes to be ready for use. Check the status of the app and continue once status is \"created\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def wait_for_mlflow_app(sagemaker_client, arn, timeout=600, poll_interval=30):\n",
    "    \"\"\"Wait for MLflow app to be ready.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < timeout:\n",
    "        app_info = sagemaker_client.describe_mlflow_app(Arn=arn)\n",
    "        status = app_info['Status']\n",
    "        \n",
    "        print(f\"MLflow app status: {status}\")\n",
    "        \n",
    "        if status in ['Created','Updated']:\n",
    "            print(\"‚úÖ MLflow app is 'Created' and ready for use!\")\n",
    "            return app_info\n",
    "        elif status in ['Failed', 'Deleted']:\n",
    "            raise Exception(f\"MLflow app failed with status: {status}\")\n",
    "        \n",
    "        time.sleep(poll_interval)\n",
    "    \n",
    "    raise TimeoutError(f\"MLflow app did not become ready within {timeout} seconds\")\n",
    "\n",
    "# Wait for the app to be ready\n",
    "app_info = wait_for_mlflow_app(sagemaker_client, mlflow_app_details['Arn'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Configure MLflow Tracking for Strands Agents\n",
    "\n",
    "Now we enable automatic logging for Strands agents so that all agent interactions, tool usage, and performance metrics are automatically captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "\n",
    "tracking_uri = mlflow_app_details['Arn']\n",
    "print(f\"MLflow App URL: {tracking_uri}\")\n",
    "\n",
    "# Set MLflow tracking URI\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = tracking_uri\n",
    "# Or you can set the tracking server as below:\n",
    "# mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "# Set experiment name and enable auto-logging\n",
    "mlflow.set_experiment(\"Strands-MLflow\")\n",
    "mlflow.strands.autolog()\n",
    "\n",
    "print(\"MLflow tracking configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run the Agent with Tracing Enabled\n",
    "\n",
    "With MLflow tracking configured and auto-logging enabled, we can now run our Strands Agent. All traces will be automatically captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def capitalize(response):\n",
    "    return response.upper()\n",
    "\n",
    "agent = Agent(model=model_sagemaker, tools=[http_request])\n",
    "agent_response = agent(\"Where is the international space station now? (Use: http://api.open-notify.org/iss-now.json)\")\n",
    "capitalize(agent_response.message['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: View Traces in MLflow UI\n",
    "\n",
    "After running the agent, traces and metrics are available in the MLflow App. You can access MLflow UI using the presigned URL below. Go to \"Strands-MLflow\" under expertiments and check on the generated trace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get presigned URL for MLflow tracking server\n",
    "presigned_response = sagemaker_client.create_presigned_mlflow_app_url(\n",
    "    Arn=mlflow_app_details['Arn'] \n",
    ")\n",
    "\n",
    "mlflow_ui_url = presigned_response['AuthorizedUrl']\n",
    "print(f\"MLflow UI URL: {mlflow_ui_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your trace will give you the details of all steps take by the agent to fulfill your request.\n",
    "<img src=\"./images/first_agent_trace.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Manual Tracing for Complete Visibility\n",
    "\n",
    "While MLflow's automatic tracing captures agent invocations and tool calls, other function calls (like our `capitalize` function) are not logged automatically.\n",
    "\n",
    "To capture the complete execution flow, we can use MLflow's manual tracing capability with the `@mlflow.trace` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@mlflow.trace(span_type=\"func\", attributes={\"operation\": \"capitalize\"})\n",
    "def capitalize(response):\n",
    "    return response.upper()\n",
    "\n",
    "@mlflow.trace\n",
    "def run_agent():\n",
    "    agent = Agent(model=model_sagemaker, tools=[http_request])\n",
    "    mlflow.update_current_trace(request_preview=\"Run Strands Agent\")\n",
    "    \n",
    "    agent_response = agent(\"Where is the international space station now? (Use: http://api.open-notify.org/iss-now.json)\")\n",
    "    capitalized_response = capitalize(agent_response.message['content'][0]['text'])\n",
    "    \n",
    "    return capitalized_response\n",
    "\n",
    "# Execute the traced function\n",
    "capitalized_response = run_agent()\n",
    "print(capitalized_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Deploying a New LLM for A/B Testing\n",
    "\n",
    "With Amazon SageMaker AI, you can optimize LLMs for your agent applications through A/B testing. This allows you to:\n",
    "\n",
    "- Deploy a new model alongside your existing one\n",
    "- Distribute traffic between both endpoints\n",
    "- Measure the impact before fully committing to an upgrade\n",
    "\n",
    "In this section, we'll upgrade from Qwen3-4B to Qwen3-8B using A/B testing.\n",
    "\n",
    "### Step 1: Deploy the New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a model from JumpStart\n",
    "model_b_name = \"qwen3-8b-model\"\n",
    "model_b_id, model_b_version = \"qwen3-8b-model\", \"1.0.0\"\n",
    "\n",
    "model_8b = JumpStartModel(\n",
    "    model_id=\"huggingface-reasoning-qwen3-8b\",  \n",
    "    model_version=\"1.0.0\",\n",
    "    name=model_b_name\n",
    ")\n",
    "model_8b.create(instance_type=\"ml.g5.2xlarge\")\n",
    "\n",
    "print(f\"Model '{model_b_name}' created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Configure Production Variants for A/B Testing\n",
    "\n",
    "We'll create production variants with 50/50 traffic split between the champion (4B) and challenger (8B) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create production variants for A/B testing\n",
    "production_variants = [\n",
    "    # The original model (champion)\n",
    "    {\n",
    "        \"VariantName\": \"qwen-4b-variant\",\n",
    "        \"ModelName\": \"qwen3-4b-model\",\n",
    "        \"InitialInstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.g5.2xlarge\",\n",
    "        \"InitialVariantWeight\": 0.5  # 50% of traffic\n",
    "    },\n",
    "    # The new model (challenger)\n",
    "    {\n",
    "        \"VariantName\": \"qwen-8b-variant\",\n",
    "        \"ModelName\": model_b_name,\n",
    "        \"InitialInstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.g5.2xlarge\",\n",
    "        \"InitialVariantWeight\": 0.5  # 50% of traffic\n",
    "    }\n",
    "]\n",
    "\n",
    "# Step 3: Create new endpoint configuration\n",
    "ENDPOINT_CONFIG_AB_TESTING = \"llm-endpoint-config-ab\"\n",
    "sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=ENDPOINT_CONFIG_AB_TESTING,\n",
    "    ProductionVariants=production_variants\n",
    ")\n",
    "\n",
    "print(f\"Endpoint config '{ENDPOINT_CONFIG_AB_TESTING}' created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Update the Endpoint with A/B Testing Configuration\n",
    "\n",
    "‚è±Ô∏è **This step takes several minutes** as the endpoint is being updated with the new configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Update the endpoint with new A/B testing configuration\n",
    "sagemaker_client.update_endpoint(\n",
    "    EndpointName=ENDPOINT_NAME,  # The endpoint name stays the same\n",
    "    EndpointConfigName=ENDPOINT_CONFIG_AB_TESTING\n",
    ")\n",
    "\n",
    "# Wait until the update is completed\n",
    "waiter = boto3.client('sagemaker').get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=ENDPOINT_NAME)\n",
    "\n",
    "print(f\"Endpoint '{ENDPOINT_NAME}' updated with A/B testing configuration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create Agents for Each Variant\n",
    "\n",
    "For controlled experiments, we create separate agents that target specific variants using the `target_variant` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands.models.sagemaker import SageMakerAIModel\n",
    "from strands import Agent, tool\n",
    "from strands_tools import http_request, calculator\n",
    "\n",
    "# Agent targeting the 4B variant (champion)\n",
    "model_sagemaker_a = SageMakerAIModel(\n",
    "    endpoint_config={\n",
    "        \"endpoint_name\": ENDPOINT_NAME,\n",
    "        \"region_name\": region,\n",
    "        \"target_variant\": \"qwen-4b-variant\"\n",
    "    },\n",
    "    payload_config={\n",
    "        \"max_tokens\": 2048,\n",
    "        \"temperature\": 0.2,\n",
    "        \"stream\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Agent targeting the 8B variant (challenger)\n",
    "model_sagemaker_b = SageMakerAIModel(\n",
    "    endpoint_config={\n",
    "        \"endpoint_name\": ENDPOINT_NAME,\n",
    "        \"region_name\": region,\n",
    "        \"target_variant\": \"qwen-8b-variant\"\n",
    "    },\n",
    "    payload_config={\n",
    "        \"max_tokens\": 2048,\n",
    "        \"temperature\": 0.2,\n",
    "        \"stream\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Variant-specific agents created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create Evaluation Dataset\n",
    "\n",
    "We create a structured evaluation dataset compatible with `mlflow.genai.evaluate()`. Each entry includes:\n",
    "\n",
    "- **inputs**: The query/prompt for the agent\n",
    "- **expectations**: Ground truth values including expected tool and expected_facts (for Correctness scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define evaluation dataset for MLflow GenAI evaluate\n",
    "# expected_facts is used by the built-in Correctness scorer (LLM judge)\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\"query\": \"Calculate 15% tip on a $85.50 restaurant bill. Use calculator tool\"},\n",
    "        \"expectations\": {\n",
    "            \"expected_tool\": \"calculator\",\n",
    "            \"expected_facts\": [\"The tip amount is $12.825 or approximately $12.82 or $12.83\", \"15% of 85.50 equals 12.825\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"query\": \"What is 2048 divided by 64? Use calculator tool\"},\n",
    "        \"expectations\": {\n",
    "            \"expected_tool\": \"calculator\",\n",
    "            \"expected_facts\": [\"The answer is 32\", \"2048 divided by 64 equals 32\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"query\": \"Calculate the square root of 144. Use calculator tool\"},\n",
    "        \"expectations\": {\n",
    "            \"expected_tool\": \"calculator\",\n",
    "            \"expected_facts\": [\"The square root of 144 is 12\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"query\": \"What is 25 multiplied by 4, then add 10? Use calculator tool\"},\n",
    "        \"expectations\": {\n",
    "            \"expected_tool\": \"calculator\",\n",
    "            \"expected_facts\": [\"The answer is 110\", \"25 times 4 is 100, plus 10 equals 110\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"query\": \"If I have $500 and spend 30%, how much do I have left? Use calculator tool\"},\n",
    "        \"expectations\": {\n",
    "            \"expected_tool\": \"calculator\",\n",
    "            \"expected_facts\": [\"$350 remaining\", \"30% of 500 is 150, so 500 minus 150 equals 350\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created evaluation dataset with {len(eval_dataset)} test cases\")\n",
    "print(\"\\nSample test case:\")\n",
    "print(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Define Scorers for MLflow Evaluation\n",
    "\n",
    "MLflow GenAI evaluation uses **scorers** to assess agent performance. We'll use a combination of custom and built-in scorers:\n",
    "\n",
    "| Scorer | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| **tool_selection_scorer** | Custom | Checks if the agent selected the correct tool for the task |\n",
    "| **Correctness** | Built-in | LLM judge that evaluates factual correctness using expected_facts |\n",
    "| **RelevanceToQuery** | Built-in | LLM judge that evaluates if response addresses the query |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import scorer, Correctness, RelevanceToQuery\n",
    "from mlflow.entities import Feedback, AssessmentSource, AssessmentSourceType\n",
    "\n",
    "@scorer\n",
    "def tool_selection_scorer(inputs: dict, outputs: str, expectations: dict) -> Feedback:\n",
    "    \"\"\"\n",
    "    Evaluates if the agent used the expected tool.\n",
    "    Checks if the expected tool name appears in the output or trace.\n",
    "    \"\"\"\n",
    "    expected_tool = expectations.get(\"expected_tool\", \"\")\n",
    "    \n",
    "    # Check if tool was mentioned in output (simplified check)\n",
    "    # In production, you'd parse the trace for actual tool calls\n",
    "    tool_used = expected_tool.lower() in outputs[\"tools\"] if outputs else False\n",
    "    \n",
    "    return Feedback(\n",
    "        name=\"tool_selection\",\n",
    "        value=1.0 if tool_used else 0.0,\n",
    "        rationale=f\"Expected tool '{expected_tool}' {'was' if tool_used else 'was NOT'} used\",\n",
    "        source=AssessmentSource(\n",
    "            source_type=AssessmentSourceType.CODE,\n",
    "            source_id=\"tool_selection_scorer_v1\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Built-in scorers from MLflow GenAI:\n",
    "# - Correctness: LLM judge that evaluates factual correctness using expected_facts\n",
    "# - RelevanceToQuery: LLM judge that evaluates if response addresses the query\n",
    "\n",
    "print(\"‚úÖ Scorers configured!\")\n",
    "print(\"   - tool_selection_scorer (custom): Checks if correct tool was used\")\n",
    "print(\"   - Correctness (built-in): LLM judge for factual correctness\")\n",
    "print(\"   - RelevanceToQuery (built-in): LLM judge for response relevance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Run MLflow GenAI Evaluation for Each Agent\n",
    "\n",
    "We use `mlflow.genai.evaluate()` to run the evaluation for each model variant. This:\n",
    "\n",
    "1. Executes the agent on each test case\n",
    "2. Applies all scorers to assess performance\n",
    "3. Logs results to MLflow for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from strands import Agent\n",
    "from strands_tools import calculator\n",
    "\n",
    "# Set experiment for A/B evaluation\n",
    "mlflow.set_experiment(\"Strands_Agents_AB_Evaluation\")\n",
    "\n",
    "# Define predict functions for each agent\n",
    "def predict_4b(query: str) -> str:\n",
    "    \"\"\"Prediction function for Qwen 4B agent\"\"\"\n",
    "    agent_model_a = Agent(model=model_sagemaker_a, tools=[calculator])\n",
    "    response = agent_model_a(query)\n",
    "    return {\"outputs\": str(response), \"tools\": list(response.metrics.tool_metrics.keys())}\n",
    "\n",
    "def predict_8b(query: str) -> str:\n",
    "    \"\"\"Prediction function for Qwen 8B agent\"\"\"\n",
    "    agent_model_b = Agent(model=model_sagemaker_b, tools=[calculator])\n",
    "    response = agent_model_b(query)\n",
    "    return {\"outputs\": str(response), \"tools\": list(response.metrics.tool_metrics.keys())}\n",
    "\n",
    "print(\"‚úÖ Agents and predict functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run evaluation for Qwen 4B variant\n",
    "print(\"Evaluating Qwen 4B variant...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "eval_results_4b = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=predict_4b,\n",
    "    scorers=[\n",
    "        tool_selection_scorer,\n",
    "        Correctness(model=\"bedrock:/us.amazon.nova-pro-v1:0\"),\n",
    "        RelevanceToQuery(model=\"bedrock:/us.amazon.nova-pro-v1:0\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Qwen 4B evaluation complete!\")\n",
    "print(f\"Run ID: {eval_results_4b.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run evaluation for Qwen 8B variant\n",
    "print(\"Evaluating Qwen 8B variant...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "eval_results_8b = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=predict_8b,\n",
    "    scorers=[\n",
    "        tool_selection_scorer,\n",
    "        Correctness(model=\"bedrock:/us.amazon.nova-pro-v1:0\"),\n",
    "        RelevanceToQuery(model=\"bedrock:/us.amazon.nova-pro-v1:0\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Qwen 8B evaluation complete!\")\n",
    "print(f\"Run ID: {eval_results_8b.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Compare Evaluation Results\n",
    "\n",
    "Let's compare the performance of both model variants across our evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get metrics from evaluation results\n",
    "metrics_4b = eval_results_4b.metrics\n",
    "metrics_8b = eval_results_8b.metrics\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = {\n",
    "    \"Metric\": [],\n",
    "    \"Qwen 4B\": [],\n",
    "    \"Qwen 8B\": [],\n",
    "    \"Winner\": []\n",
    "}\n",
    "\n",
    "for metric_name in metrics_4b.keys():\n",
    "    val_4b = metrics_4b.get(metric_name, 0)\n",
    "    val_8b = metrics_8b.get(metric_name, 0)\n",
    "    \n",
    "    comparison_data[\"Metric\"].append(metric_name)\n",
    "    comparison_data[\"Qwen 4B\"].append(f\"{val_4b:.3f}\" if isinstance(val_4b, float) else val_4b)\n",
    "    comparison_data[\"Qwen 8B\"].append(f\"{val_8b:.3f}\" if isinstance(val_8b, float) else val_8b)\n",
    "    \n",
    "    if isinstance(val_4b, (int, float)) and isinstance(val_8b, (int, float)):\n",
    "        if val_4b > val_8b:\n",
    "            comparison_data[\"Winner\"].append(\"4B ‚úì\")\n",
    "        elif val_8b > val_4b:\n",
    "            comparison_data[\"Winner\"].append(\"8B ‚úì\")\n",
    "        else:\n",
    "            comparison_data[\"Winner\"].append(\"Tie\")\n",
    "    else:\n",
    "        comparison_data[\"Winner\"].append(\"-\")\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"A/B TESTING EVALUATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Get presigned URL\n",
    "response = sagemaker_client.create_presigned_mlflow_app_url(\n",
    "    Arn=mlflow_app_details['Arn'],\n",
    "    ExpiresInSeconds=300\n",
    ")\n",
    "presigned_url = response['AuthorizedUrl']\n",
    "\n",
    "# Get run info\n",
    "run_4b = mlflow.get_run(eval_results_4b.run_id)\n",
    "run_8b = mlflow.get_run(eval_results_8b.run_id)\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "## üìä View A/B Evaluation Comparison in MLflow\n",
    "\n",
    "üîó **[Open MLflow UI]({presigned_url})**\n",
    "\n",
    "Once authenticated, select the **Evaluations** tab and compare these runs:\n",
    "\n",
    "| Run Name | Run ID |\n",
    "|----------|--------|\n",
    "| {run_4b.info.run_name} | `{eval_results_4b.run_id[:8]}...` |\n",
    "| {run_8b.info.run_name} | `{eval_results_8b.run_id[:8]}...` |\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/mlflow_eval_compare.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Transition to the New Model (Optional)\n",
    "\n",
    "Based on the evaluation results, if the new model performs better, you can transition by adjusting the variant weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to transition fully to the 8B model if it performs better\n",
    "\"\"\"\n",
    "production_variants = [\n",
    "    {\n",
    "        \"VariantName\": \"qwen-8b-variant\",\n",
    "        \"ModelName\": model_b_name,\n",
    "        \"InitialInstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.g5.2xlarge\",\n",
    "        \"InitialVariantWeight\": 1\n",
    "    }\n",
    "]\n",
    "\n",
    "ENDPOINT_CONFIG_QWEN3_8b = \"llm-endpoint-config-qwen3-8b\"\n",
    "sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=ENDPOINT_CONFIG_QWEN3_8b,\n",
    "    ProductionVariants=production_variants\n",
    ")\n",
    "sagemaker_client.update_endpoint(\n",
    "    EndpointName=ENDPOINT_NAME,  # The endpoint name stays the same\n",
    "    EndpointConfigName=ENDPOINT_CONFIG_QWEN3_8b\n",
    ")\n",
    "\n",
    "# Wait until the update is completed\n",
    "waiter = boto3.client('sagemaker').get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=ENDPOINT_NAME)\n",
    "\n",
    "# validate that new model with a Strands agent. \n",
    "model_sagemaker = SageMakerAIModel(\n",
    "    endpoint_config={\n",
    "        \"endpoint_name\": ENDPOINT_NAME,\n",
    "        \"region_name\": region\n",
    "    },\n",
    "    payload_config={\n",
    "        \"max_tokens\": 2048,\n",
    "        \"temperature\": 0.2,\n",
    "        \"stream\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Test the agent with SageMaker model\n",
    "agent = Agent(model=model_sagemaker, tools=[http_request], callback_handler=None)\n",
    "agent(\"Where is the international space station now? (Use: http://api.open-notify.org/iss-now.json)\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Troubleshooting\n",
    "\n",
    "### Common Issues with MLflow Tracing\n",
    "\n",
    "If you encounter `ImportError: cannot import name 'TokenUsageKey' from 'mlflow.tracing.constant'` or other tracing issues:\n",
    "\n",
    "1. **Check MLflow version**: Should be 3.4.0 or greater\n",
    "2. **Verify IAM permissions**: Your role needs access to:\n",
    "   - Read, write, list the S3 bucket used as the artifact location\n",
    "   - Access MLflow tracking server\n",
    "\n",
    "### Verify MLflow Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "assert mlflow.__version__ >= \"3.4.0\", \"Please upgrade MLflow to version 3.4.0 or greater\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Cleanup\n",
    "\n",
    "‚ö†Ô∏è **Important**: Run this section to delete the resources created in this notebook and avoid ongoing charges.\n",
    "\n",
    "This will delete:\n",
    "- SageMaker endpoint\n",
    "- Endpoint configurations\n",
    "- MLflow tracking server\n",
    "- Local evaluation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment  to delete created resources\n",
    "\"\"\"import os\n",
    "\n",
    "# Delete the endpoint\n",
    "sagemaker_client.delete_endpoint(EndpointName=ENDPOINT_NAME)\n",
    "print(f\"Deleted endpoint: {ENDPOINT_NAME}\")\n",
    "\n",
    "# Delete endpoint configurations\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=INITIAL_CONFIG_NAME)\n",
    "print(f\"Deleted endpoint config: {INITIAL_CONFIG_NAME}\")\n",
    "\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=ENDPOINT_CONFIG_AB_TESTING)\n",
    "print(f\"Deleted endpoint config: {ENDPOINT_CONFIG_AB_TESTING}\")\n",
    "\n",
    "# Delete MLflow tracking server\n",
    "sagemaker_client.delete_mlflow_app(\n",
    "    Arn=mlflow_app_details[\"Arn\"]\n",
    ")\n",
    "print(f\"Deleted MLflow app: {mlflow_app_details['Arn']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Cleanup completed successfully!\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored how to build AI agents using Strands Agents SDK with models deployed on Amazon SageMaker AI endpoints, while leveraging SageMaker Managed MLflow for comprehensive agent observability and evaluation.\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. **Built a basic Strands Agent** using Amazon Bedrock as the model provider\n",
    "2. **Deployed a Qwen model** from SageMaker JumpStart as an inference endpoint\n",
    "3. **Integrated SageMaker models** with Strands Agents for greater infrastructure control\n",
    "4. **Set up MLflow observability** for automatic agent tracing and monitoring\n",
    "5. **Implemented A/B testing with evaluation** comparing model variants using:\n",
    "   - Tool Selection Accuracy\n",
    "   - Task Completion Rate\n",
    "   - Response Latency\n",
    "\n",
    "### Key Evaluation Metrics\n",
    "\n",
    "| Metric | Description | Use Case |\n",
    "|--------|-------------|----------|\n",
    "| Tool Selection | Verifies the agent used the expected tool (e.g., calculator) | Agent capability assessment |\n",
    "| Correctness | Checks if response contains expected facts | Accuracy measurement |\n",
    "| Relevance to Query | Evaluates if response directly addresses the user's question | Response quality assessment |\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To continue your journey with Strands Agents and SageMaker AI:\n",
    "\n",
    "- **[Amazon SageMaker AI Documentation](https://docs.aws.amazon.com/sagemaker/)** - Learn more about model deployment and management\n",
    "- **[Strands Agents SDK](https://github.com/strands-agents/strands-agents)** - Explore how to build and customize AI agents\n",
    "- **[Strands Evaluation Guide](https://strandsagents.com/latest/documentation/docs/user-guide/observability-evaluation/evaluation/)** - Deep dive into agent evaluation\n",
    "- **[SageMaker AI MLflow](https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow.html)** - Dive deeper into agent observability using SageMaker AI MLflow App\n",
    "- **[MLflow GenAI Evaluation](https://mlflow.org/docs/latest/genai/eval-monitor.html)** - Learn about MLflow's evaluation capabilities\n",
    "\n",
    "---\n",
    "\n",
    "**Happy building!** üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
